<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <meta name="description" content="ÂÆâÁøî's home page">
    <title>ÂÆâÁøî's Homepage</title>
    <link rel="icon" href="assets/img/profile_pic.jpg" type="image/jpg">
    <link href="./profile.css" rel="stylesheet" type="text/css">
    <style>
        body {
            font-family: Georgia, serif;
            color: #111;
            width: 850px;
        }

        .profile-pic {
            width: 180px;
            border-radius: 50%;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
        }

        .profile-pic:hover {
            transform: scale(1.04);
        }

        .top-section {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .contact-box {
            background: #f9f9f9;
            border-radius: 8px;
            padding: 10px 12px;
            display: inline-block;
            margin-top: 0.6em;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.05);
            font-size: 0.95em;
        }

        .contact-box img {
            width: 16px;
            vertical-align: middle;
            margin-right: 6px;
            opacity: 0.9;
        }

        .contact-box a {
            margin-right: 16px;
        }

        .emoji {
            font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, sans-serif;
            font-size: 1.1em;
        }

        .publication-entry {
            display: flex;
            align-items: flex-start;
            background: #fdfdfd;
            padding: 12px;
            margin: 15px 0;
            border-radius: 10px;
            border: 1px solid #eee;
            box-shadow: 0 2px 6px rgba(0, 0, 0, 0.03);
        }

        .publication-entry img {
            width: 200px;
            height: 100px;
            border-radius: 8px;
            margin-right: 16px;
            object-fit: cover;
        }

        .pub-list {
            list-style: none;
            padding-left: 0;
        }

        h1,
        h2 {
            color: #263054;
        }

        h2 {
            border-bottom: 1px solid #aaa;
            padding-bottom: 5px;
            margin-top: 40px;
        }
    </style>
</head>

<body>

    <div id="layout-content" style="margin: 25px; max-width: 800px;">
        <div class="top-section">
            <div style="max-width: 70%;">
                <h1>Xiang An</h1>
                <h3><span class="emoji">üéì</span> Algorithm Engineer</h3>
<!--
                <p style="margin-bottom: 0.4em;">
                    <img src="assets/img/school.png" alt="University"
                        style="width:16px; vertical-align:middle; margin-right:5px;">
                    <a href="https://www.cqu.edu.cn/" target="_blank">Chongqing University</a>
                </p>
                <p style="margin-top: 0;">
                    <img src="assets/img/location.png" alt="Location"
                        style="width:16px; vertical-align:middle; margin-right:5px;">
                    Chongqing, China
                </p> -->

                <div class="contact-box">
                    <img src="assets/img/envelope.png" alt="Email">
                    <a href="mailto:anxiangsir@outlook.com">anxiangsir@outlook.com</a>

                    <img src="assets/img/google.png" alt="Google Scholar">
                    <a href="https://scholar.google.com.hk/citations?user=1ckaPgwAAAAJ&hl=en" target="_blank">Google Scholar</a>

                    <img src="assets/img/github.png" alt="GitHub">
                    <a href="https://github.com/anxiangsir" target="_blank">GitHub</a>
                </div>
            </div>
            <div>
                <img class="profile-pic" src="assets/img/profile_pic.jpg" alt="Profile Picture">
            </div>
        </div>

        <h2>About Me</h2>
        <p>
        I am an <b>Algorithm Researcher</b> and <b>Team Lead</b> of the Multimodal Large Model Group at <b>GlintLab</b>, focusing on computer vision and multimodal large models.
        </p>

        <h2>Selected Publications</h2>
        <ul class="pub-list">
            <li>
                <div class="publication-entry">
                    <div>
                        <b>Llava-OneVision-1.5: Fully open framework for democratized multimodal training</b><br>
                        <span style="color: #777;">
                            <span style="color: #297be6; font-weight: bold; text-decoration: underline;">Xiang An</span>, Yin Xie, Kaicheng Yang,
                            Wenkang Zhang, Ziyong Feng, Ziwei Liu, Bo Li, Jiankang Deng
                        </span><br>
                        <b>Preprint, 2025</b><br>
                        [<a href="https://arxiv.org/abs/2509.23661">Paper</a>] [<a href="https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5">Code</a>]
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry">
                    <div>
                        <b>UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning</b><br>
                        <span style="color: #777;">
                            Tiancheng Gu, Kaicheng Yang, Kaichen Zhang,
                            <span style="color: #297be6; font-weight: bold; text-decoration: underline;">Xiang An</span>,
                            Ziyong Feng, Yueyi Zhang, Weidong Cai, Jiankang Deng, Lidong Bing
                        </span><br>
                        <b>AAAI, 2026 (Oral)</b><br>
                        [<a href="https://arxiv.org/pdf/2510.13515">Paper</a>] [<a href="https://github.com/GaryGuTC/UniME-v2">Code</a>]
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry">
                    <div>
                        <b>Region-based Cluster Discrimination for Visual Representation Learning</b><br>
                        <span style="color: #777;">
                            Yin Xie, Kaicheng Yang, <span style="color: #297be6; font-weight: bold; text-decoration: underline;">Xiang An (Project Leader)</span>,
                            Kun Wu, Yongle Zhao, Weimo Deng, Zimin Ran, Yumeng Wang,
                            Ziyong Feng, Roy Miles, Ismail Elezi, Jiankang Deng
                        </span><br>
                        <b>ICCV, 2025 (Highlight)</b><br>
                        [<a href="https://arxiv.org/abs/2507.20025">Paper</a>] [<a href="https://github.com/deepglint/unicom">Code</a>]
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry">
                    <div>
                        <b>Multi-label Cluster Discrimination for Visual Representation Learning</b><br>
                        <span style="color: #777;">
                            <span style="color: #297be6; font-weight: bold; text-decoration: underline;">Xiang An</span>, Kaicheng Yang, Xiangzi Dai, Ziyong Feng, Jiankang Deng
                        </span><br>
                        <b>ECCV, 2024</b><br>
                        [<a href="https://arxiv.org/abs/2407.17331">Paper</a>] [<a href="https://github.com/deepglint/unicom">Code</a>]
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry">
                    <div>
                        <b>Unicom: Universal and Compact Representation Learning for Image Retrieval</b><br>
                        <span style="color: #777;">
                            <span style="color: #297be6; font-weight: bold; text-decoration: underline;">Xiang An</span>, Jiankang Deng, Kaicheng Yang, Jiawei Li,
                            Ziyong Feng, Jia Guo, Jing Yang, Tongliang Liu
                        </span><br>
                        <b>ICLR, 2023</b><br>
                        [<a href="https://arxiv.org/abs/2304.05884">Paper</a>] [<a href="https://github.com/deepglint/unicom">Code</a>]
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry">
                    <div>
                        <b>Killing Two Birds with One Stone: Efficient and Robust Training of Face Recognition CNNs by Partial FC</b><br>
                        <span style="color: #777;">
                            <span style="color: #297be6; font-weight: bold; text-decoration: underline;">Xiang An</span>, Jiankang Deng, Jia Guo, Ziyong Feng,
                            Xuhan Zhu, Jing Yang, Tongliang Liu
                        </span><br>
                        <b>CVPR, 2022</b><br>
                        [<a href="https://arxiv.org/abs/2203.15565">Paper</a>] [<a href="https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch">Code</a>]
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry">
                    <div>
                        <b>Partial FC: Training 10 million identities on a single machine</b><br>
                        <span style="color: #777;">
                            <span style="color: #297be6; font-weight: bold; text-decoration: underline;">Xiang An</span>, Xuhan Zhu, Yuan Gao, Yang Xiao, Yongle Zhao,
                            Ziyong Feng, Lan Wu, Bin Qin, Ming Zhang, Debing Zhang, Ying Fu
                        </span><br>
                        <b>ICCVW, 2021</b><br>
                        [<a href="https://arxiv.org/abs/2010.05222">Paper</a>] [<a href="https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch">Code</a>]
                    </div>
                </div>
            </li>
        </ul>

        <h2>Community Contribution</h2>
        <p>I actively contribute to open-source projects in face recognition, representation learning, and multimodal large models. I am the <b>#2 contributor</b> to the <b>InsightFace</b> ecosystem (~27k‚≠ê), and co-maintain several influential vision and multimodal repositories.</p>

        <ul class="pub-list">
            <li>
                <div class="publication-entry">
                    <div>
                        <b>InsightFace</b> &middot; 2D &amp; 3D Face Analysis Toolkit
                        <br>
                        <span style="color: #777;">
                            Major contributor (#2 by contributions) to the core InsightFace ecosystem for large-scale face recognition and analysis.
                        </span>
                        <br>
                        <span style="color: #555;">Code:</span>
                        <a href="https://github.com/deepinsight/insightface" target="_blank">deepinsight/insightface</a> ‚≠ê27k+
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry">
                    <div>
                        <b>LLaVA-OneVision-1.5</b> &middot; Multimodal Training Framework
                        <br>
                        <span style="color: #777;">
                            Fully open framework for democratized multimodal training, advancing large multimodal models (LMMs) for unified vision-language understanding. I am the leader of this project.
                        </span>
                        <br>
                        <span style="color: #555;">Code:</span>
                        <a href="https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5" target="_blank">EvolvingLMMs-Lab/LLaVA-OneVision-1.5</a> ‚≠ê600+
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry">
                    <div>
                        <b>LLaVA-NeXT</b> &middot; Next-Generation LMMs
                        <br>
                        <span style="color: #777;">
                            Contributed to the vision module of LLaVA-NeXT, enhancing its OCR capability, optimized the visual encoder and training pipeline for text-rich images.
                        </span>
                        <br>
                        <span style="color: #555;">Code:</span>
                        <a href="https://github.com/LLaVA-VL/LLaVA-NeXT" target="_blank">LLaVA-VL/LLaVA-NeXT</a> ‚≠ê4000+
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry">
                    <div>
                        <b>UNICOM</b> &middot; Universal Representation for Image Retrieval
                        <br>
                        <span style="color: #777;">
                            Author and maintainer of <b>Unicom</b>, a universal and compact representation learning framework for large-scale image retrieval.
                        </span>
                        <br>
                        <span style="color: #555;">Code:</span>
                        <a href="https://github.com/deepglint/unicom" target="_blank">deepglint/unicom</a> ‚≠ê600+
                    </div>
                </div>
            </li>
        </ul>


        <h2>Awards & Competitions</h2>
        <ul style="list-style-type: none; padding-left: 0;">
            <li><span class="emoji">üèÖ</span> <b>ICCV 2025 Outstanding Reviewer</b></li>
            <li><span class="emoji">üèÖ</span> <b>CVPR 2024 Outstanding Reviewer</b></li>
            <li><span class="emoji">üèÜ</span> <b>Randed 1st in NIST FRVT Competition, Visa Track 1:1</b></li>
            <li><span class="emoji">üèÜ</span> <b>2024 ‰∏≠ÂõΩÂπ¥Â∫¶ÂäõÈáè‰∫∫Áâ©ÊèêÂêç</b></li>
            <li><span class="emoji">üèÜ</span> <b>Ranked 1st in the graduate entrance examination (major)</b></li>
            <li><span class="emoji">üèÜ</span> <b>First Place in Vehicle Re-Identification, PRCV 2019</b></li>
        </ul>

    </div>
</body>

</html>