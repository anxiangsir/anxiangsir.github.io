<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ÂÆâÁøî's home page">
    <title>ÂÆâÁøî's Homepage</title>
    <link rel="icon" href="assets/img/profile_pic.jpg" type="image/jpg">
    <style>
        :root {
            --primary-color: #297be6;
            --text-dark: #1f2937;
            --text-gray: #4b5563;
            --text-light: #6b7280;
            --bg-page: #f3f4f6;
            --bg-card: #ffffff;
            --border-color: #e5e7eb;
            --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
            --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            color: var(--text-dark);
            background-color: var(--bg-page);
            margin: 0;
            padding: 40px 20px;
            line-height: 1.6;
        }

        /* ‰∏ªÂÆπÂô®ÔºöÂ¢ûÂä†Á∫∏Âº†Ë¥®ÊÑü */
        #layout-content {
            margin: 0 auto;
            max-width: 900px;
            background-color: var(--bg-card);
            padding: 48px;
            border-radius: 16px;
            box-shadow: var(--shadow-lg);
        }

        /* Â§¥ÈÉ®Âå∫Âüü */
        .top-section {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            margin-bottom: 40px;
            gap: 24px;
        }

        .profile-info h1 {
            font-size: 2.5rem;
            margin: 0 0 8px 0;
            color: #111827;
            letter-spacing: -0.025em;
        }

        .profile-info h3 {
            font-weight: 500;
            color: var(--text-gray);
            margin: 0 0 16px 0;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .profile-pic {
            width: 180px;
            height: 180px;
            border-radius: 50%;
            object-fit: cover;
            box-shadow: var(--shadow-md);
            border: 4px solid #fff;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            flex-shrink: 0;
        }

        .profile-pic:hover {
            transform: scale(1.02) rotate(2deg);
            box-shadow: var(--shadow-lg);
        }

        /* ËÅîÁ≥ªÊñπÂºèÊ°Ü‰ºòÂåñ */
        .contact-box {
            display: flex;
            flex-wrap: wrap;
            gap: 12px;
            margin-top: 16px;
        }

        .contact-link {
            display: inline-flex;
            align-items: center;
            background: #f9fafb;
            border: 1px solid var(--border-color);
            padding: 6px 12px;
            border-radius: 20px;
            font-size: 0.9em;
            color: var(--text-gray);
            text-decoration: none;
            transition: all 0.2s;
        }

        .contact-link:hover {
            background: #eff6ff;
            border-color: var(--primary-color);
            color: var(--primary-color);
        }

        .contact-link img {
            width: 16px;
            height: 16px;
            margin-right: 8px;
            opacity: 0.8;
        }

        /* Ê†áÈ¢òÊ†∑Âºè */
        h2 {
            font-size: 1.5rem;
            color: #111827;
            border-bottom: 2px solid var(--border-color);
            padding-bottom: 12px;
            margin-top: 48px;
            margin-bottom: 24px;
        }

        p {
            color: var(--text-gray);
            margin-bottom: 16px;
        }

        b {
            color: var(--text-dark);
            font-weight: 600;
        }

        /* ÂàóË°®Ê†∑Âºè */
        .pub-list {
            list-style: none;
            padding: 0;
            display: flex;
            flex-direction: column;
            gap: 20px;
        }

        /* Âç°ÁâáÊ†∑Âºè‰ºòÂåñ */
        .publication-entry {
            display: flex;
            padding: 20px;
            border-radius: 12px;
            background: #fff;
            border: 1px solid var(--border-color);
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }

        .publication-entry:hover {
            transform: translateY(-2px);
            box-shadow: var(--shadow-md);
            border-color: #d1d5db;
        }

        .publication-entry img {
            width: 180px;
            height: 100px;
            border-radius: 8px;
            margin-right: 20px;
            object-fit: cover;
            background-color: #f3f4f6; /* ÂõæÁâáÂä†ËΩΩÂâçÁöÑÂç†‰ΩçËâ≤ */
            border: 1px solid #f3f4f6;
            flex-shrink: 0;
        }

        .pub-content {
            flex: 1;
        }

        .pub-title {
            font-size: 1.1rem;
            font-weight: 700;
            color: #111827;
            margin-bottom: 6px;
            display: block;
            line-height: 1.4;
        }

        .pub-authors {
            color: var(--text-light);
            font-size: 0.95rem;
            margin-bottom: 6px;
            display: block;
        }

        .pub-venue {
            color: #000;
            font-weight: 600;
            font-size: 0.9rem;
            margin-bottom: 10px;
            display: inline-block;
        }

        /* ËÉ∂ÂõäÊåâÈíÆÊ†∑Âºè [Paper] [Code] */
        .link-badges {
            display: flex;
            gap: 8px;
            margin-top: 4px;
        }

        .badge-link {
            display: inline-flex;
            align-items: center;
            padding: 4px 10px;
            font-size: 0.8rem;
            font-weight: 600;
            color: var(--primary-color);
            background-color: #eff6ff;
            border-radius: 6px;
            text-decoration: none;
            transition: background-color 0.2s;
        }

        .badge-link:hover {
            background-color: #dbeafe;
            text-decoration: none;
        }

        /* È´ò‰∫Æ‰ΩúËÄÖÂêç */
        .me-highlight {
            color: var(--primary-color);
            font-weight: 600;
            text-decoration: underline;
            text-underline-offset: 2px;
        }

        /* Â•ñÈ°πÂàóË°® */
        .awards-list li {
            padding: 8px 0;
            display: flex;
            align-items: center;
            color: var(--text-dark);
        }

        .awards-list .emoji {
            margin-right: 12px;
            font-size: 1.2em;
        }

        /* ÁßªÂä®Á´ØÈÄÇÈÖç */
        @media (max-width: 768px) {
            #layout-content {
                padding: 24px 20px;
                width: auto;
                margin: 0;
                border-radius: 0;
                box-shadow: none;
            }

            body {
                padding: 0;
                background-color: #fff;
            }

            .top-section {
                flex-direction: column-reverse;
                align-items: center;
                text-align: center;
            }

            .profile-info {
                max-width: 100% !important;
            }

            .profile-info h3 {
                justify-content: center;
            }

            .contact-box {
                justify-content: center;
            }

            .publication-entry {
                flex-direction: column;
            }

            .publication-entry img {
                width: 100%;
                height: auto;
                margin-right: 0;
                margin-bottom: 16px;
            }
        }
    </style>
</head>

<body>

    <div id="layout-content">
        <div class="top-section">
            <div class="profile-info" style="max-width: 70%;">
                <h1>Xiang An (ÂÆâÁøî)</h1>
                <h3><span class="emoji">üéì</span> Algorithm Engineer</h3>
                <div class="contact-box">
                    <a href="mailto:anxiangsir@outlook.com" class="contact-link">
                        <img src="assets/img/envelope.png" alt="Email">Email
                    </a>
                    <a href="https://scholar.google.com.hk/citations?user=1ckaPgwAAAAJ&hl=en" target="_blank" class="contact-link">
                        <img src="assets/img/google.png" alt="Google Scholar">Scholar
                    </a>
                    <a href="https://github.com/anxiangsir" target="_blank" class="contact-link">
                        <img src="assets/img/github.png" alt="GitHub">GitHub
                    </a>
                </div>
            </div>
            <div>
                <img class="profile-pic" src="assets/img/profile_pic.jpg" alt="Xiang An">
            </div>
        </div>

        <h2>About Me</h2>
        <p>
            I am an <b>Research Scientist</b> and <b>Team Lead</b> of the Multimodal Large Model Group at <b>GlintLab</b>, focusing on computer vision and multimodal large models.
        </p>
        <p>
            Currently, our team is building the <b>next-generation Vision Transformer (ViT)</b> to address some of the most urgent needs in modern MLLMs.
        </p>

        <h2>Selected Publications</h2>
        <ul class="pub-list">
            <li>
                <div class="publication-entry">
                    <!-- ËøôÈáåÁöÑÂõæÁâá src Â¶ÇÊûúÊòØÁ©∫ÁöÑÊàñËÄÖÂä†ËΩΩÂ§±Ë¥•‰ºöÊòæÁ§∫ËÉåÊôØËâ≤Âç†‰Ωç -->
                    <img src="assets/img/publication_preview_1.jpg" alt="Paper Preview" onerror="this.style.display='none'">
                    <div class="pub-content">
                        <span class="pub-title">Llava-OneVision-1.5: Fully open framework for democratized multimodal training</span>
                        <span class="pub-authors">
                            <span class="me-highlight">Xiang An</span>, Yin Xie, Kaicheng Yang, Wenkang Zhang, Ziyong Feng, Ziwei Liu, Bo Li, Jiankang Deng
                        </span>
                        <span class="pub-venue">Preprint, 2025</span>
                        <div class="link-badges">
                            <a href="https://arxiv.org/abs/2509.23661" class="badge-link">Paper</a>
                            <a href="https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5" class="badge-link">Code</a>
                        </div>
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry">
                    <img src="assets/img/publication_preview_2.jpg" alt="Paper Preview" onerror="this.style.display='none'">
                    <div class="pub-content">
                        <span class="pub-title">UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning</span>
                        <span class="pub-authors">
                            Tiancheng Gu, Kaicheng Yang, Kaichen Zhang, <span class="me-highlight">Xiang An</span>, Ziyong Feng, Yueyi Zhang, Weidong Cai, Jiankang Deng, Lidong Bing
                        </span>
                        <span class="pub-venue">AAAI, 2026 (Oral)</span>
                        <div class="link-badges">
                            <a href="https://arxiv.org/pdf/2510.13515" class="badge-link">Paper</a>
                            <a href="https://github.com/GaryGuTC/UniME-v2" class="badge-link">Code</a>
                        </div>
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry">
                    <img src="assets/img/publication_preview_3.jpg" alt="Paper Preview" onerror="this.style.display='none'">
                    <div class="pub-content">
                        <span class="pub-title">Region-based Cluster Discrimination for Visual Representation Learning</span>
                        <span class="pub-authors">
                            Yin Xie, Kaicheng Yang, <span class="me-highlight">Xiang An (Project Leader)</span>, Kun Wu, Yongle Zhao, Weimo Deng, Zimin Ran, Yumeng Wang, Ziyong Feng, Roy Miles, Ismail Elezi, Jiankang Deng
                        </span>
                        <span class="pub-venue">ICCV, 2025 (Highlight)</span>
                        <div class="link-badges">
                            <a href="https://arxiv.org/abs/2507.20025" class="badge-link">Paper</a>
                            <a href="https://github.com/deepglint/unicom" class="badge-link">Code</a>
                        </div>
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry">
                    <img src="assets/img/publication_preview_4.jpg" alt="Paper Preview" onerror="this.style.display='none'">
                    <div class="pub-content">
                        <span class="pub-title">Multi-label Cluster Discrimination for Visual Representation Learning</span>
                        <span class="pub-authors">
                            <span class="me-highlight">Xiang An</span>, Kaicheng Yang, Xiangzi Dai, Ziyong Feng, Jiankang Deng
                        </span>
                        <span class="pub-venue">ECCV, 2024</span>
                        <div class="link-badges">
                            <a href="https://arxiv.org/abs/2407.17331" class="badge-link">Paper</a>
                            <a href="https://github.com/deepglint/unicom" class="badge-link">Code</a>
                        </div>
                    </div>
                </div>
            </li>

             <li>
                <div class="publication-entry">
                    <img src="assets/img/publication_preview_5.jpg" alt="Paper Preview" onerror="this.style.display='none'">
                    <div class="pub-content">
                        <span class="pub-title">Unicom: Universal and Compact Representation Learning for Image Retrieval</span>
                        <span class="pub-authors">
                             <span class="me-highlight">Xiang An</span>, Jiankang Deng, Kaicheng Yang, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, Tongliang Liu
                        </span>
                        <span class="pub-venue">ICLR, 2023</span>
                        <div class="link-badges">
                            <a href="https://arxiv.org/abs/2304.05884" class="badge-link">Paper</a>
                            <a href="https://github.com/deepglint/unicom" class="badge-link">Code</a>
                        </div>
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry">
                    <img src="assets/img/publication_preview_6.jpg" alt="Paper Preview" onerror="this.style.display='none'">
                    <div class="pub-content">
                        <span class="pub-title">Killing Two Birds with One Stone: Efficient and Robust Training of Face Recognition CNNs by Partial FC</span>
                        <span class="pub-authors">
                            <span class="me-highlight">Xiang An</span>, Jiankang Deng, Jia Guo, Ziyong Feng, Xuhan Zhu, Jing Yang, Tongliang Liu
                        </span>
                        <span class="pub-venue">CVPR, 2022</span>
                        <div class="link-badges">
                            <a href="https://arxiv.org/abs/2203.15565" class="badge-link">Paper</a>
                            <a href="https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch" class="badge-link">Code</a>
                        </div>
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry">
                    <img src="assets/img/publication_preview_7.jpg" alt="Paper Preview" onerror="this.style.display='none'">
                    <div class="pub-content">
                        <span class="pub-title">Partial FC: Training 10 million identities on a single machine</span>
                        <span class="pub-authors">
                            <span class="me-highlight">Xiang An</span>, Xuhan Zhu, Yuan Gao, Yang Xiao, Yongle Zhao, Ziyong Feng, Lan Wu, Bin Qin, Ming Zhang, Debing Zhang, Ying Fu
                        </span>
                        <span class="pub-venue">ICCVW, 2021</span>
                        <div class="link-badges">
                            <a href="https://arxiv.org/abs/2010.05222" class="badge-link">Paper</a>
                            <a href="https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch" class="badge-link">Code</a>
                        </div>
                    </div>
                </div>
            </li>
        </ul>

        <h2>Community Contribution</h2>
        <p>I actively contribute to open-source projects in face recognition, representation learning, and multimodal large models. I am the <b>#2 contributor</b> to the <b>InsightFace</b> ecosystem (~27k‚≠ê), and co-maintain several influential vision and multimodal repositories.</p>

        <ul class="pub-list">
            <li>
                <div class="publication-entry" style="align-items: center;">
                    <div class="pub-content">
                        <span class="pub-title">InsightFace ¬∑ 2D & 3D Face Analysis Toolkit</span>
                        <span class="pub-authors">
                            Major contributor (#2 by contributions) to the core InsightFace ecosystem for large-scale face recognition and analysis.
                        </span>
                        <div class="link-badges" style="margin-top: 8px;">
                            <a href="https://github.com/deepinsight/insightface" class="badge-link" target="_blank">‚≠ê 27k+ Stars</a>
                            <a href="https://github.com/deepinsight/insightface" class="badge-link" style="background-color: transparent; border: 1px solid var(--border-color); color: var(--text-dark);">View Repo</a>
                        </div>
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry" style="align-items: center;">
                    <div class="pub-content">
                        <span class="pub-title">LLaVA-OneVision-1.5 ¬∑ Multimodal Training Framework</span>
                        <span class="pub-authors">
                            Fully open framework for democratized multimodal training, advancing large multimodal models (LMMs).
                        </span>
                        <div class="link-badges" style="margin-top: 8px;">
                             <a href="https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5" class="badge-link" target="_blank">‚≠ê 600+ Stars</a>
                             <a href="https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5" class="badge-link" style="background-color: transparent; border: 1px solid var(--border-color); color: var(--text-dark);">View Repo</a>
                        </div>
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry" style="align-items: center;">
                    <div class="pub-content">
                        <span class="pub-title">LLaVA-NeXT ¬∑ Next-Generation LMMs</span>
                        <span class="pub-authors">
                            Contributed to the vision module of LLaVA-NeXT, enhancing its OCR capability, optimized the visual encoder and training pipeline for text-rich images.
                        </span>
                        <div class="link-badges" style="margin-top: 8px;">
                             <a href="https://github.com/LLaVA-VL/LLaVA-NeXT" class="badge-link" target="_blank">‚≠ê 4000+ Stars</a>
                             <a href="https://github.com/LLaVA-VL/LLaVA-NeXT" class="badge-link" style="background-color: transparent; border: 1px solid var(--border-color); color: var(--text-dark);">View Repo</a>
                        </div>
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry" style="align-items: center;">
                    <div class="pub-content">
                        <span class="pub-title">UNICOM ¬∑ Universal Representation for Image Retrieval</span>
                        <span class="pub-authors">
                            Author and maintainer of <b>Unicom</b>, a universal and compact representation learning framework for large-scale image retrieval.
                        </span>
                        <div class="link-badges" style="margin-top: 8px;">
                             <a href="https://github.com/deepglint/unicom" class="badge-link" target="_blank">‚≠ê 600+ Stars</a>
                             <a href="https://github.com/deepglint/unicom" class="badge-link" style="background-color: transparent; border: 1px solid var(--border-color); color: var(--text-dark);">View Repo</a>
                        </div>
                    </div>
                </div>
            </li>

            <li>
                <div class="publication-entry" style="align-items: center;">
                    <div class="pub-content">
                        <span class="pub-title">Urban Seg ¬∑ Remote Sensing Semantic Segmentation</span>
                        <span class="pub-authors">
                            A beginner-friendly repository for remote sensing semantic segmentation. It allows training with pre-trained models using just a single code file.
                        </span>
                        <div class="link-badges" style="margin-top: 8px;">
                             <a href="https://github.com/anxiangsir/urban_seg" class="badge-link" target="_blank">‚≠ê 460+ Stars</a>
                             <a href="https://github.com/anxiangsir/urban_seg" class="badge-link" style="background-color: transparent; border: 1px solid var(--border-color); color: var(--text-dark);">View Repo</a>
                        </div>
                    </div>
                </div>
            </li>
        </ul>

        <h2>Awards & Competitions</h2>
        <ul class="awards-list" style="list-style-type: none; padding-left: 0;">
            <li><span class="emoji">üèÖ</span> <b>ICCV 2025 Outstanding Reviewer</b></li>
            <li><span class="emoji">üèÖ</span> <b>CVPR 2024 Outstanding Reviewer</b></li>
            <li><span class="emoji">üèÜ</span> <b>Randed 1st in NIST FRVT Competition, Visa Track 1:1</b></li>
            <li><span class="emoji">üèÜ</span> <b>2024 ‰∏≠ÂõΩÂπ¥Â∫¶ÂäõÈáè‰∫∫Áâ©ÊèêÂêç</b></li>
            <li><span class="emoji">üèÜ</span> <b>Ranked 1st in the graduate entrance examination (major)</b></li>
            <li><span class="emoji">üèÜ</span> <b>First Place in Vehicle Re-Identification, PRCV 2019</b></li>
        </ul>

    </div>
</body>

</html>